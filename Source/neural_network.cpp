/********************************************************************************************/
/*                                                                                          */
/*   ANN-Backpropagation: A C++ implementation of Artificial Neural Network (ANN)           */
/*                        and backpropagation algorithm for classification                  */
/*                                                                                          */
/*   N E U R A L   N E T W O R K   C L A S S                                                */
/*                                                                                          */
/*   Avinash Ranganath                                                                      */
/*   Robotics Lab, Department of Systems Engineering and Automation                         */
/*   University Carlos III of Mardid(UC3M)                                                  */
/*   Madrid, Spain                                                                          */
/*   E-mail: nash911@gmail.com                                                              */
/*   https://sites.google.com/site/anashranga/                                              */
/*                                                                                          */
/********************************************************************************************/

#include "neural_network.h"

// CONSTRUCTOR

/// Constructs a neural network architecture.
/// Initializes learning rate parameter ùõº and regularization parameter Œª to defaults.
/// Creates and randomly initializes matrices Œò‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1).
/// @param inputNeurons Number of neurons in the input layer.
/// @param hiddenLayers Number of hidded layers, and hidden neurons in each hidden layer.
/// @param outputNeurons Number of neurons in the output layer.

NeuralNetwork::NeuralNetwork(const unsigned int& inputNeurons, const vector<unsigned int>& hiddenLayers, const unsigned int& outputNeurons)
{
    //--Default hyper-parameters values--//
    d_alpha = ALPHA;
    d_lamda = LAMDA;

    d_inputneurons.set_size(inputNeurons);
    d_outputneurons.set_size(outputNeurons);

    for(unsigned int hl = 0; hl < hiddenLayers.size(); hl++)
    {
        vec hidLayer(hiddenLayers[hl]);
        d_hiddenneurons.push_back(hidLayer);
    }

    unsigned int rows = 0;
    unsigned int cols = 0;
    unsigned int layers = networkArchitecture().size();
    double epsilon_init = 0.1;

    cout << endl << "Weight matrices dimensions:" << endl;
    for(unsigned int l = 0; l < (layers - 1); l++)
    {
        rows = networkArchitecture()[l+1];
        cols = networkArchitecture()[l]+1;

        cout << endl << "Layer " << l+1 <<  " weight matrix:    Rows: " << rows << "    Cols: " << cols << endl;

        //--Random initialization of Theta matrix Œò‚ÅΩl‚Åæ--//
        epsilon_init = sqrt(6.0) / sqrt(rows + cols);
        mat theta(rows, cols, fill::randn);
        d_Theta.push_back(theta * 2.0 * epsilon_init - epsilon_init);
    }
}


void NeuralNetwork::set_alpha(const double& alpha)
{
    if(alpha < 0.0)
    {
        cerr << "ANN-Backpropagation Error: NeuralNetwork class." << endl
             << "void set_alpha(const double) methon." << endl
             << "Parameter alpha: " << alpha << " must be >= 0."
             << endl;

        exit(1);
    }

    d_alpha = alpha;
}


void NeuralNetwork::set_lamda(const double& lamda)
{
    if(lamda < 0.0)
    {
        cerr << "ANN-Backpropagation Error: NeuralNetwork class." << endl
             << "void set_lamda(const double) methon." << endl
             << "Parameter lamda: " << lamda << " must be >= 0."
             << endl;

        exit(1);
    }

    d_lamda = lamda;
}


double NeuralNetwork::alpha(void) const
{
    return d_alpha;
}


double NeuralNetwork::lamda(void) const
{
    return d_lamda;
}


// vector<unsigned int> networkArchitecture(void) const

/// Returns a vector containing network architecture size.
/// Number of layers in the network is the size of the vector.
/// Number of neurons in each layer is stored in each row of the vector.

vector<unsigned int> NeuralNetwork::networkArchitecture(void) const
{
    vector<unsigned int> architecture;

    architecture.push_back(d_inputneurons.n_rows);
    for(unsigned int hl=0; hl<d_hiddenneurons.size(); hl++)
    {
        architecture.push_back(d_hiddenneurons[hl].n_rows);
    }
    architecture.push_back(d_outputneurons.n_rows);

    return architecture;
}


// vec Layer(const unsigned int) const

/// Returns a vector of activations a‚ÅΩl‚Åæ, where l=1,2,...,L, of layer l.
/// The activations are from the most recent forward-propagation of the network.
/// @param l Layer of the network.

vec NeuralNetwork::Layer(const unsigned int l) const
{
    if(l >= networkArchitecture().size())
    {
        cerr << "ANN-Backpropagation Error: NeuralNetwork class." << endl
             << "vec Layer(const unsigned int) methon." << endl
             << "Layer l: " << l << " should be < network architecture size: " << networkArchitecture().size()
             << endl;

        exit(1);
    }

    vec a;
    vec a_0 = ones<vec>(1);

    if(l == 0)
    {
        a = d_inputneurons;

        //--Adding element a_0 ‚àÄ l‚â†L--//
        a.insert_rows(0, a_0);
    }
    else if(l == networkArchitecture().size() - 1)
    {
        a = d_outputneurons;
    }
    else
    {
        a = d_hiddenneurons[l-1];

        //--Adding element a_0 ‚àÄ l‚â†L--//
        a.insert_rows(0, a_0);
    }

    return a;
}


// vector<mat> Theta(void) const

/// Returns the vector of Theta matrices Œò‚ÅΩl‚Åæ ‚àÄ l ‚àà [1,2,...,(L-1)].

vector<mat> NeuralNetwork::Theta(void) const
{
    return d_Theta;
}


// mat Theta(const unsigned int&) const

/// Returns a Theta matrices Œò‚ÅΩl‚Åæ of later l.
/// @param l Layer of the network.

mat NeuralNetwork::Theta(const unsigned int& l) const
{
    if(l >= networkArchitecture().size())
    {
        cerr << "ANN-Backpropagation Error: NeuralNetwork class." << endl
             << "mat Theta(const unsigned int&) const method." << endl
             << "Layer l: " << l << " should be < network architecture size: " << networkArchitecture().size()
             << endl;

        exit(1);
    }
    return d_Theta[l];
}


// void printTheta(void) const

/// Prints on screen Œò‚ÅΩl‚Åæ ‚àÄ l=1,2,...,(L-1).

void NeuralNetwork::printTheta(void) const
{
    for(unsigned int l=0; l<d_Theta.size(); l++)
    {
        cout << endl << "Theta_" << l+1 << ":" << endl;
        d_Theta[l].print();
    }
}


// vec activation(const vec&, const unsigned int&)

/// Calculates activation a‚ÅΩl‚Åæ = g(z), for a given layer l ‚àà [1,2,...,L].
/// Stores vector a‚ÅΩl‚Åæ in layer l of the network.
/// Returns a vector of activations a‚ÅΩl‚Åæ for layer l.
/// @param inputVec Vector v of inputs to calculate activation. v = x_i where l=1, or v = a‚ÅΩl-1‚Åæ where l ‚àà [2,3,...,L]
/// @param l Layer of the network.

vec NeuralNetwork::activation(const vec& inputVec, const unsigned int& l)
{
    if(l >= networkArchitecture().size())
    {
        cerr << "ANN-Backpropagation Error: NeuralNetwork class." << endl
             << "vec activation(const vec, const unsigned int) methon." << endl
             << "Layer l: " << l << " should be < network architecture size: " << networkArchitecture().size()
             << endl;

        exit(1);
    }

    //--a‚ÅΩl‚Åæ = x_i if l=1--//
    vec a = inputVec;

    //--If l ‚àà [2,3,4,...,L]--//
    if(l > 0)
    {
        //--a‚ÅΩl‚Åæ = g(Œò‚ÅΩl‚Åæ * a‚ÅΩl-1‚Åæ) ‚àÄ l ‚àà [2,3,...,L]--//
        a = sigmoid(vec(d_Theta[l-1] * a));
    }

    //--Storing a‚ÅΩl‚Åæ in layer l of the neural network--//
    if(l == 0)
    {
        d_inputneurons = a;
    }
    else if(l < networkArchitecture().size() - 1)
    {
        d_hiddenneurons[l-1] = a;
    }
    else
    {
        d_outputneurons = a;
    }


    //--Adding bias neuron a‚ÅΩl‚Åæ_0--//
    if(l < (networkArchitecture().size() - 1))
    {
        //--Adding element a‚ÅΩl‚Åæ_0 ‚àÄ l‚â†L--//
        vec a_0 = ones(1);
        a.insert_rows(0, a_0);
    }

    return a;
}


// mat activation(const mat&, const unsigned int&)

/// Calculates activation a‚ÅΩl‚Åæ = g(z), for a given layer l ‚àà [1,2,...,L].
/// Stores vector a‚ÅΩl‚Åæ in layer l of the network.
/// Returns a matrix of activations a‚ÅΩl‚Åæ for layer l.
/// @param inputMat Matrix X of inputs to calculate activation. //v = x_i where l=1, or v = a‚ÅΩl-1‚Åæ where l ‚àà [2,3,...,L]
/// @param l Layer of the network.

mat NeuralNetwork::activation(const mat& inputMat, const unsigned int& l)
{
    if(l >= networkArchitecture().size())
    {
        cerr << "ANN-Backpropagation Error: NeuralNetwork class." << endl
             << "mat activation(const mat&, const unsigned int&) methon." << endl
             << "Layer l: " << l << " should be < network architecture size: " << networkArchitecture().size()
             << endl;

        exit(1);
    }

    //--a‚ÅΩl‚Åæ = X if l=1--//
    mat a = inputMat;

    //--If l ‚àà [2,3,4,...,L]--//
    if(l > 0)
    {
        //--a‚ÅΩl‚Åæ = g(Œò‚ÅΩl‚Åæ * a‚ÅΩl-1‚Åæ) ‚àÄ l ‚àà [2,3,...,L]--//
        a = sigmoid(mat(a * d_Theta[l-1].t()));
    }

    //--Adding bias neuron a‚ÅΩl‚Åæ_0--//
    if(l < (networkArchitecture().size() - 1))
    {
        //--Adding element a‚ÅΩl‚Åæ_0 ‚àÄ l‚â†L--//
        //vec a_0 = ones(m);
        vec a_0 = ones<vec>(a.n_rows);
        a.insert_cols(0, a_0);
    }

    return a;
}


// vec sigmoid(const vec&) const

/// Calculates sigmoid for a given vector.
/// Returns a vector of sigmoids z‚ÅΩl‚Åæ for layer l.
/// @param z Input vector z = Œò‚ÅΩl‚Åæ * a‚ÅΩl-1‚Åæ where l=2,3,...,L

vec NeuralNetwork::sigmoid(const vec& z) const
{
    vec one = ones<vec>(z.n_rows);

    //--g(z) = ¬π/(1 + e^-z)--//
    return(one/(one + exp(-z)));

}


// mat sigmoid(const mat&) const

/// Calculates sigmoid for a given matrix.
/// Returns a matrix of sigmoids z‚ÅΩl‚Åæ for layer l.
/// @param Z Input matrix Z = Œò‚ÅΩl‚Åæ * a‚ÅΩl-1‚Åæ where l=2,3,...,L

mat NeuralNetwork::sigmoid(const mat& Z) const
{
    mat one = ones<mat>(Z.n_rows, Z.n_cols);

    //--g(Z) = ¬π/(1 + e^-Z)--//
    return(one/(one + exp(-Z)));

}


// vec h_Theta(const vec&)

/// Calculates hŒò(x) = g(z‚ÅΩL‚Åæ)
/// Returns a vector v ‚àà R^k, predicting one instance of data x_i.
/// @param x Input vector x, which is a feature vector in the data set.

vec NeuralNetwork::h_Theta(const vec& x)
{
    vec a = x;
    unsigned int layers = networkArchitecture().size();

    for(unsigned int l=0; l<layers; l++)
    {
        a = activation(a, l);
    }

    return(a);
}


// mat h_Theta(const mat&)

/// Calculates hŒò(x) = g(z‚ÅΩL‚Åæ)
/// Returns a matrix A ‚àà R^(m x k), predicting data in X.
/// @param X Input matrix X, of input features in the data set.

mat NeuralNetwork::h_Theta(const mat& X)
{
    mat A = X;
    unsigned int layers = networkArchitecture().size();

    for(unsigned int l=0; l<layers; l++)
    {
        A = activation(A, l);
    }

    return(A);
}


// double cost(const vector<mat>&, const mat&, const mat&)

/// Calculates and returns the regularized cost of the neural network, given Œò, input matric X and k-class label matric Y.
/// @param Theta A vector of Theta matrices Œò‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1).
/// @param X Input feature matrix X.
/// @param Y Input label matrix Y.

double NeuralNetwork::cost(const vector<mat>& Theta, const mat& X, const mat& Y)
{
    double regu = 0;

    unsigned int m = X.n_rows;
    mat one = ones<mat>(m, Y.col(0).n_rows);
    mat h_X;
    mat theta;

    //--Error--//
    //-- m K                                                      --//
    //-- ‚àë ‚àë y‚ÅΩi‚Åæ_k log(hŒò(x‚ÅΩi‚Åæ)_k) + (1-y‚ÅΩi‚Åæ_k) log(1-hŒò(x‚ÅΩi‚Åæ)_k)--//
    //-- i k                                                      --//
    h_X = h_Theta(X);
    mat error = (Y.t() % log(h_X)) + ((one - Y.t()) % log(one - h_X));
    //--Note: '%' is element-wise multiplication in Armadillo--//

    //--Regularization--//
    //-- L-1 s_l s_(l+1)           --//
    //--  ‚àë   ‚àë     ‚àë   (Œò‚ÅΩl‚Åæ_ij)¬≤ --//
    //--  l   i     j              --//
    for(unsigned int l=0; l<Theta.size(); l++)
    {
        theta = Theta[l].cols(1, Theta[l].n_cols-1);
        regu = regu + sum(sum(theta % theta));
    }

    //--Cost Function--//
    //--           1             Œª                  --//
    //-- J(Œò) = - --- (error) + ---- (Regulization) --//
    //--           m             2m                 --//
    double cost = -(accu(error)/m) + ((d_lamda/(2.0*m))*regu);

    return(cost);
}


// void train(const mat&, const mat&)

/// Trains the neural network through backpropagation and gradient descent, given input matric X and k-class-label matric Y.
/// @param X Training set feature matrix X.
/// @param Y Training set label matrix Y.

void NeuralNetwork::train(const mat& X, const mat& Y)
{
    if(X.n_rows != Y.n_cols)
    {
        cerr << "ANN-Backpropagation Error: NeuralNetwork class." << endl
             << "void train(const mat, const mat) methon." << endl
             << "Rows of feature matrix X: " << X.n_rows << " should be equal to colums of label matrix Y: " << Y.n_cols
             << endl;

        exit(1);
    }

    vector<mat> partDeriv_cost;
    double c=0;
    double c_prev=0;
    unsigned int it=0;

    fstream costGraph;
    costGraph.open("../Output/cost.dat", ios_base::out);
    costGraph << "#Iteration  #Cost" << endl;

    do
    {
        c_prev = c;
        c = cost(d_Theta, X, Y);
        cout << endl << "Iteration: " << it << "   Cost: " << c << endl;

        //--Cost function graph file update--//
        costGraph << it++ << " " << c << endl;

        //--Computer partial derivatives of the cost function J(Œò) w.r.t Œò‚ÅΩl‚Åæ_ij ‚àÄ l,i,j--//
        //--      ‚àÇ          --//
        //--  --------- J(Œò) --//
        //--  ‚àÇ Œò‚ÅΩl‚Åæ_ij      --//
        partDeriv_cost = backpropagate(d_Theta, X, Y);

        //--Computer gradient of Œò‚ÅΩl‚Åæ_ij ‚àÄ l,i,j, w.r.t J(Œò), and compair it with gradients calculated--//
        //--by backprop to ensure the code is bug-free--//
        //gradientCheck(d_Theta, partDeriv_cost, X, Y);


        //--                       |‚Äæ    ‚àÇ      ‚Äæ|--//
        //-- Œò‚ÅΩl‚Åæ_ij := Œò‚ÅΩl‚Åæ_ij - ùõº| --------J(Œò)|--//
        //--                       |_‚àÇŒò‚ÅΩl‚Åæ_ij   _|--//
        gradientdescent(d_Theta, partDeriv_cost);

    }while(fabs(c_prev - c) > LEARNING_CURVE_DELTA);

    costGraph.close();
}


// void gradientdescent(vector<mat>&, const vector<mat>&)

/// Implements gradient descent algorithm for updating Œò‚ÅΩl‚Åæ_ij ‚àÄ l,i,j, give partial derivatives D‚ÅΩl‚Åæ_ij ‚àÄ l,i,j
/// @param Theta A vector of Theta matrices Œò‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1)
/// @param D A vector of partial derivatives matrices D‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1)

void NeuralNetwork::gradientdescent(vector<mat>& Theta, const vector<mat>& D)
{
    for(unsigned int l=0; l<Theta.size(); l++)
    {
        //--Œò‚ÅΩl‚Åæ_ij := Œò‚ÅΩl‚Åæ_ij - ùõº(‚àÇJ(Œò)/‚àÇŒò‚ÅΩl‚Åæ_ij)--//
        Theta[l] = Theta[l] - (d_alpha * D[l]);
    }
}


// vector<mat> backpropagate(vector<mat>&, const mat&, const mat&)

/// Implements backpropagate algorithm for calculating partial derivatives D‚ÅΩl‚Åæ_ij ‚àÄ l,i,j
/// Returns a vector of partial derivative matrices D‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1)
/// @param Theta A vector of Theta matrices Œò‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1)
/// @param X Input feature matrix X.
/// @param Y Input label matrix Y.

vector<mat> NeuralNetwork::backpropagate(const vector<mat>& Theta, const mat& X, const mat& Y)
{
    //--D‚ÅΩl‚Åæ, where l = 1,2,3,...,(L-1)--//
    vector<mat> D = Theta;

    //--Œî‚ÅΩl‚Åæ, where l = 1,2,3,...,(L-1)--//
    vector<mat> Delta;

    //--Œ¥‚ÅΩl‚Åæ, where l = 2,3,...,(L-1),L--//
    vector<vec> delta(networkArchitecture().size());

    vec a_L;
    vec y_i;
    vec a_l;

    unsigned int m = X.n_rows;
    unsigned int L = networkArchitecture().size()-1;

    //--Set D‚ÅΩl‚Åæ_ij = 0, ‚àÄ l,i,j--//
    //--Set Œî‚ÅΩl‚Åæ_ij = 0, ‚àÄ l,i,j--//
    for(unsigned int l=0; l<Theta.size(); l++)
    {
        //--D‚ÅΩl‚Åæ--//
        D[l].zeros();

        //--Œî‚ÅΩl‚Åæ--//
        mat Del = Theta[l];
        Del.zeros();
        Delta.push_back(Del);
    }

    for(unsigned int i=0; i<m; i++)
    {
        //--Forward-Propagation--//
        a_L = h_Theta(vec(X.row(i).t()));
        y_i = Y.col(i);

        //--Œ¥‚ÅΩL‚Åæ = a‚ÅΩL‚Åæ - y‚ÅΩi‚Åæ--//
        delta[L] = a_L - y_i;

        //--Compute Œ¥‚ÅΩL-1‚Åæ, Œ¥‚ÅΩL-2‚Åæ,...,Œ¥‚ÅΩ2‚Åæ--//
        for(unsigned int l=L-1; l>0; l--)
        {
            delta[l] = error(Theta[l], delta[l+1], l);
        }

        for(unsigned int l=0; l<L; l++)
        {
            a_l = Layer(l);

            //--Œî‚ÅΩl‚Åæ := Œî‚ÅΩl‚Åæ + (Œ¥‚ÅΩl+1‚Åæ * a‚ÅΩl‚Åæ')--//
            Delta[l] = Delta[l] + (delta[l+1] * a_l.t());
        }
    }

    //--Normalize over batch size and regularize--//
    vector<mat> Theta_clone = Theta;
    for(unsigned int l=0; l<L; l++)
    {
        //--Œò‚ÅΩl‚Åæ_ij := 0 ‚àÄ j=0--//
        Theta_clone[l].col(0).zeros();
        //cout << endl << "SET BIAS COL TO ZERO" << endl;

        //--D‚ÅΩl‚Åæ := (¬π/m) Œî‚ÅΩl‚Åæ + ŒªŒò‚ÅΩl‚Åæ--//
        D[l] = ((1.0/m) * Delta[l]) + (d_lamda * Theta_clone[l]);
        //cout << endl << "NORMALIZED AND REGULARIZED" << endl;
    }

    //cout << endl << "RETURNING FROM backpropagate()" << endl;
    return D;
}


// vec error(const mat&, const vec&, const unsigned int&) const

/// Computes error vector Œ¥‚ÅΩl‚Åæ, where l=2,3,...,L, given Theta matrix Œò‚ÅΩl‚Åæ and input vector v = Œ¥‚ÅΩl+1‚Åæ, if l < L, or v = hŒò(x) if l = L
/// Returns a vector of error terms for layer l.
/// @param Theta matrix Œò‚ÅΩl‚Åæ.
/// @param input Input vector v = Œ¥‚ÅΩl+1‚Åæ, if l < L, or v = hŒò(x) if l = L.
/// @param l Layer of the network.

vec NeuralNetwork::error(const mat& Theta, const vec& input, const unsigned int& l) const
{
    if(l >= networkArchitecture().size())
    {
        cerr << "ANN-Backpropagation Error: NeuralNetwork class." << endl
             << "vec error(const mat, const vec, const unsigned int) methon." << endl
             << "Layer l: " << l << " should be < network architecture size: " << networkArchitecture().size()
             << endl;

        exit(1);
    }

    vec delta_l;
    vec a_l = Layer(l);

    //--Œ¥‚ÅΩl‚Åæ where l=L--//
    if(l == networkArchitecture().size() - 1)
    {
        //--Œ¥‚ÅΩL‚Åæ = a‚ÅΩL‚Åæ - y‚ÅΩi‚Åæ--//
        delta_l = a_l - input;
        return delta_l;
    }
    //--Œ¥‚ÅΩl‚Åæ where l‚â†L--//
    else
    {
        vec one = ones<vec>(Layer(l).n_rows);

        //--Œ¥‚ÅΩl‚Åæ = (Œò‚ÅΩl‚Åæ)'Œ¥‚ÅΩl+1‚Åæ .* g'(z‚ÅΩl‚Åæ)--//
        //--Here -> g'(z‚ÅΩl‚Åæ) = a‚ÅΩl‚Åæ .* (1-a‚ÅΩl‚Åæ) --//
        delta_l = (Theta.t() * input) % (a_l % (one - a_l));

        //--Ommiting Œ¥‚ÅΩl‚Åæ_0, which is the error term of the bias node--//
        return delta_l.rows(1, delta_l.n_rows-1);
    }
}


// void gradientCheck(const vector<mat>&, const vector<mat>&, const mat&, const mat&)

/// Implements gradient checking by calculating different between partial derivatives calculated numerically and by through backpropagation.
/// @param Theta A vector of Theta matrices Œò‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1)
/// @param D_backprop A vector of partial derivatives matrices D‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1), calculated using backpropagation.
/// @param X Input feature matrix X.
/// @param Y Input label matrix Y.

void NeuralNetwork::gradientCheck(const vector<mat>& Theta, const vector<mat>& D_backprop, const mat& X, const mat& Y)
{
    vector<mat> D_num;

    D_num = numericalGradient(Theta, X, Y);

    cout << endl;
    for(unsigned int l=0; l<Theta.size(); l++)
    {
        mat diff = D_backprop[l] - D_num[l];
        cout << "Max derivative difference in Theta_" << l+1 << ": " << max(max(diff[l])) << "  ";
    }
    cout << endl;
}


// vector<mat> numericalGradient(const vector<mat>&, const mat&, const mat&)

/// Calculates partial derivatives D‚ÅΩl‚Åæ_ij ‚àÄ l,i,j, numerically.
/// Returns a vector of numerically calculated partial derivative matrices D‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1)
/// @param Theta A vector of Theta matrices Œò‚ÅΩl‚Åæ ‚àÄ l = 1,2,...,(L-1)
/// @param X Input feature matrix X.
/// @param Y Input label matrix Y.

vector<mat> NeuralNetwork::numericalGradient(const vector<mat>& Theta, const mat& X, const mat& Y)
{
    //--D‚ÅΩl‚Åæ, where l = 1,2,3,...,(L-1)--//
    vector<mat> D = Theta;

    unsigned int L = networkArchitecture().size()-1;
    double e = 0.001;

    for(unsigned int l=0; l<L; l++)
    {
        for(unsigned int i=0; i<D[l].n_rows; i++)
        {
            for(unsigned int j=0; j<D[l].n_cols; j++)
            {
                vector<mat> Theta_minus_e = Theta;
                vector<mat> Theta_plus_e = Theta;

                Theta_minus_e[l](i,j) = Theta[l](i,j) - e;
                Theta_plus_e[l](i,j) = Theta[l](i,j) + e;

                D[l](i,j) = (cost(Theta_minus_e, X, Y) - cost(Theta_plus_e, X, Y)) / (2.0*e);
            }
        }
    }

    return D;
}


// double test(const mat&, const mat&, const vec&)

/// Runs test on the given test set, and returns the cost.
/// @param X Test set feature matrix.
/// @param target Test set target matrix.
/// @param k Class size of the dataset.

double NeuralNetwork::test(const mat& X, const mat& target, const vec& labels, const bool show_stats=false)
{
    unsigned int k = labels.n_rows;
    double c = cost(d_Theta, X, target);
    umat confMat = confusionMatrix(X, target, k);

    if(show_stats)
    {
        print_confusionMatrix(confMat, labels);
    }

    return c;
}


// mat predict(const mat& X, const unsigned int&)

/// Given an input feature matrix, predicts the most likely class label
/// of the input through forward-propagation
/// @param X Test set feature matrix.
/// @param k Class size of the dataset.

mat NeuralNetwork::predict(const mat& X, const unsigned int& k)
{
    unsigned int m = X.n_rows;

    mat h_X = h_Theta(X);
    mat P = zeros<mat>(m, k);

    ucolvec max_indx = index_max(h_X, 1);

    for(unsigned int i=0; i<m; i++)
    {
        P(i, max_indx[i]) = 1.0;
    }

    return P;
}


// umat confusionMatrix(const mat&, const mat&, const unsigned int&)

/// Given an input feature matrix, predicts the most likely class label
/// of the input through forward-propagation
/// @param X Test set feature matrix.
/// @param target Test set target matrix.
/// @param k Class size of the dataset.

umat NeuralNetwork::confusionMatrix(const mat& X, const mat& target, const unsigned int& k)
{
    umat confMat(k, k);
    confMat.zeros();

    mat predicted_y = predict(X, k);

    unsigned int m = X.n_rows;
    uvec row_indx;
    uvec col_indx;

    for(unsigned int i=0; i<m; i++)
    {
        row_indx = find(target.col(i) == 1, 1);
        col_indx = find(predicted_y.row(i) == 1, 1);

        confMat(row_indx, col_indx) += 1;
    }

    urowvec col_sum = sum(confMat, 0);
    ucolvec row_sum = sum(confMat, 1);

    uvec TP = confMat.diag();
    uvec TN = accu(confMat) - (col_sum.t() + row_sum) + TP ;
    uvec FP = sum(confMat, 0).t() - TP;
    uvec FN = sum(confMat, 1) - TP;

    /*mat binartConfMat = zeros<mat>(2, 2);
    binartConfMat(0,0) = TP(1);
    binartConfMat(0,1) = confMat(0,1);
    binartConfMat(1,0) = confMat(1,0);
    binartConfMat(1,1) = TP(0);*/

    return confMat;
}


// void print_confusionMatrix(umat, const vec&) const


void NeuralNetwork::print_confusionMatrix(umat confMat, const vec& labels) const
{
    unsigned int k = labels.n_rows;
    unsigned int total_samples = accu(confMat);
    unsigned int total_TP = trace(confMat);

    uvec actuals = conv_to< uvec >::from(labels);
    confMat.insert_cols(0, actuals);

    cout << endl << "confMat.size(): " << confMat.size() << endl;

    cout << endl << "       Confusion Matrix        "
         << endl << "     --------------------      "
         << endl << "          Predicted            "
         << endl << "        <----------->          "
         << endl << "                         ";

    for(unsigned int i=0; i<k; i++)
    {
        cout << i << "            ";
    }

    cout << endl << "                        ";
    for(unsigned int i=0; i<k; i++)
    {
        cout << "---          ";
    }

    cout << endl << confMat << endl;
    cout << endl << "Total Samples: " << total_samples
         << endl << "Total True Positives: " << total_TP;

    /*cout << endl << "No. of actual positives: " << TP + FN
         << endl << "No. of actual negatives: " << TN + FP
         << endl << "Total Misclassifications: " << FP + FN << endl;*/
}
